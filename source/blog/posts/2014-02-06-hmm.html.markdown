---
layout: post
title: 一千个韩梅梅
published: false
---

其实，这篇是对隐马尔科夫模型(HMM)的一个粗浅的总结。
跟韩梅梅没有直接的关系，嗯(hmm)。

### 隐马尔科夫(HMM)

对于一系列观察，假设每个观察背后隐藏着某个变量(类别，状态；后文都叫状态了)。
HMM假设

* 每个在某个状态下不变
* 某个状态只和前一状态相关

根据上面的假设，一个观察序列$x$与它的状态序列$y$的联合概率为。

$$
p(x,y)=p(x|y)p(y)=p(y_1) \prod_{i=2}^T{p(y_i|y_{i-1})} \prod_{i=1}^T{p(x_i|y_i)}
$$

READMORE

像分词、词性标注、chuncking等等很多任务都可以当成序列标注来做。
还有一些更神奇的任务，比如[词对齐](http://acl.ldc.upenn.edu/C/C96/C96-2141.pdf)，也是可以用序列标注来建模的。

HMM对观察和状态的序列建模。序列的概率由$p(y\_1)$，$p(x_t\|y_t)$ 和$p(y_{t+1}\|y_t)$决定。

### 有监督HMM

有模型，有数据，就有参数估计问题。
对于HMM来讲，要估计的参数包括

* $p(y_1)$: 初始概率
* $p(x_t\|y_t)$: 发射概率
* $p(y_{t+1}\|y_t)$: 转移概率

如果数据中含有标准的状态序列$y$，我们希望我们估计出的模型尽可能与数据相符。
这种相符用似然$\mathcal{L}$来刻画。
$\mathcal{L}(\Theta)=\sum_{i=1}^m{\log p(y^i, x^i|\Theta)}$。

最大化$\mathcal{L}$，就可以得到一个比较好的参数。
具体实施方法是对似然求导数，

### 无监督HMM

如果数据中不含隐含状态的序列，我们还是能从中学出一星半点东西来。
因为没法估计每个实例的联合概率$p(y,x)$，只能估计出这个实例出现的可能性。
如何估计这个可能性呢？
枚举所有状态序列$y$然后求和。也就是$p(x)=\sum_y{p(x,y)}$可以得到这个实例$x$出现的可能性。
也就是求出$x$的边缘概率。

在这种情况下，似然可以写成
$$\mathcal{L}(\Theta)=\sum\_{i=1}^m{\log{p(x\^i|\Theta)}}=\sum\_{i=1}^m{\log{\sum\_y{p(x\^i,y|\Theta)}}}$$

要求进行参数估计，从这个似然函数出发，不能得到解析解。
但可以用EM算法近似得到一个局部最优。


#### 前向-后向过程

我们要对$p(x\_1,...,x\_{t-1},y\_t=j)$建模，
使用链式法则和马尔科夫假设可以得到推导形式的概率表示。

$$
\begin{array}{ll}
p(x\_1,...,x\_{t-1},y\_t=j) & =\sum\_k{p(x\_1,...,x\_{t-2},x\_{t-1},y\_{t-1}=k,y\_t=j)} \\\\
 & =\sum\_k{p(x\_1,...,x\_{t-2},y\_{t-1}=k) p(x\_{t-1},y\_t=j|x\_1,...,x\_{t-2},y\_{t-1}=k)} \\\\
 & (\text{apply markov assumption}) \\\\
 & =\sum\_k{p(x\_1,...,x\_{t-2},y\_{t-1}=k) p(x\_{t-1},y\_t=j|y\_{t-1}=k)} \\\\
 & =\sum\_k{p(x\_1,...,x\_{t-2},y\_{t-1}=k) p(y\_t=j|y\_{t-1}=k) p(x\_t|y\_{t=j})}
\end{array}
$$

令$\alpha\_j(t)=p(x\_1,...,x\_{t-1},y\_t=j)$，
上面的式子可以写成$\alpha\_j(t)=\sum\_k{\alpha\_k(t-1) p(y\_t=j|y\_{t-1}=k) p(x\_t|y\_{t=j})}$

后向过程同理，推导$p(x\_t,...,x\_n|y\_t=j)$。

$$
\begin{array}{ll}
p(x\_t,...,x\_n|y\_t=j) &= \sum\_k{p(x\_t,...,x\_n,y\_{t+1}=k|y\_t=j)} \\\\
&=\sum\_k{p(x\_{t+1},...,x\_n|x\_t,y\_{t+1}=k,y\_t=j) p(x\_t,y\_{t+1}=k|y\_t=j) }\\\\
&=\sum\_k{p(x\_{t+1},...,x\_n|x\_t,y\_{t+1}=k,y\_t=j) p(x\_t|y\_{t+1}=k,y\_t=j) p(y\_{t+1}=k|y\_t=j) }\\\\
& (\text{apply markov assumption}) \\\\
&=\sum\_k{p(x\_{t+1},...,x\_n|y\_{t+1}=k) p(x\_t|y\_t=j) p(y\_{t+1}=k|y\_t=j) }\\\\
\end{array}
$$

令$\beta\_j(t)=p(x\_t,...,x\_n|y\_t=j)$，
得到$\beta\_j(t)=\sum\_k{\beta\_k(t+1) p(x\_t|y\_t=j) p(y\_{t+1}=k|y\_t=j) }$
#### 使用对数

### Gibbs采样HMM


