<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Static Oneplus</title>
  <subtitle>不可控制论</subtitle>
  <id>http://yjliu.net/blog</id>
  <link href="http://yjliu.net/blog"/>
  <link href="http://yjliu.net/feed.xml" rel="self"/>
  <updated>2013-08-04T17:07:11Z</updated>
  <author>
    <name>Oneplus</name>
  </author>
  <entry>
    <title>研一这一年吧</title>
    <link rel="alternate" href="http://yjliu.net/blog/2013/08/05/summary-on-first-year-as-graduate-student.html"/>
    <id>http://yjliu.net/blog/2013/08/05/summary-on-first-year-as-graduate-student.html</id>
    <published>2013-08-04T17:07:11Z</published>
    <updated>2014-02-23T15:06:00+08:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;给这一年列一个时间表吧，这篇文章想写很久了，虽然有预感写出来又是满满的负能量。&lt;/p&gt;

&lt;h3 id="section"&gt;2012年&lt;/h3&gt;

&lt;h4 id="section-1"&gt;8月-9月&lt;/h4&gt;
&lt;p&gt;写了一篇叫《基于序列标注的中文分词、词性标注模型比较分析》，投了一个学生会议。大概是想论证用分类方法做分词、词性标注这些序列性问题可以取得与序列标注模型类似的性能。还想强调分类速度比较快。不过实验并没有获得符合预期的结果。所以这篇论文的论点比较奇怪，不管是写还是修改都很痛苦。在后来被转投中文信息学报，我又不得不痛苦地改了一遍。&lt;/p&gt;

&lt;h4 id="section-2"&gt;9月-10月&lt;/h4&gt;
&lt;p&gt;参加了微博分词的评测。最后提交的系统是一个混了一大坨预处理的特征、半监督特征的CRF模型。除了把别人论证过的东西实现了一通后，几乎没有引入什么有新意的东西。由于官方没公布评测排名，并不知道自己的系统排名如何，但开会时统计了一下，大概是第二的位置。大概半年后，我review这个系统时发现当时使用crfsuite工具训练模型时没把负特征开关打开，所以最终结果应该是高于提交的系统的结果的。但主办方也没公布数据，也没法去做实验。&lt;/p&gt;

&lt;h4 id="section-3"&gt;10月-11月&lt;/h4&gt;
&lt;p&gt;拿出不多的时间学习了机器学习，包括实现了一些简单的机器学习算法。重组织了博客。调研了一部分domain adaptation的论文，改之前的水文。还有一些杂七杂八的上课实验什么的。&lt;/p&gt;

&lt;h4 id="section-4"&gt;11月-12月&lt;/h4&gt;
&lt;p&gt;写微博分词的评测报告，同时也看一段gibbs抽样。准备去上海开会的报告以及去开会，接着改论文投中文信息学报。&lt;/p&gt;

&lt;h4 id="section-5"&gt;12月-1月&lt;/h4&gt;
&lt;p&gt;前半段考试，做了一坨课程实验，后半段去天津开了微博分词的会，其间都是准备poster一些杂事。这月后半段开始接了网管的工作。这其间还草草做了一个字聚类帮助分词的实验。后来这个非常烂的idea投了ccl2013。&lt;/p&gt;

&lt;h3 id="section-6"&gt;2013年&lt;/h3&gt;

&lt;h4 id="section-7"&gt;1月-2月&lt;/h4&gt;
&lt;p&gt;前半段修硬盘来着。刚接手网管，实验室的磁盘阵列就挂了。大概原因就是raid5坏了两块后没人知道，第三块坏后就彻底挂了。大冬天抱着硬盘跑数据恢复，反正是非常苦逼。后来十天草草准备了一下托福，一战成了挂逼。&lt;/p&gt;

&lt;h4 id="section-8"&gt;2月-3月&lt;/h4&gt;
&lt;p&gt;打算考G，利用过年时间背了一个多月单词，约了5月的G和6月的T。&lt;/p&gt;

&lt;h4 id="section-9"&gt;3月-5月&lt;/h4&gt;
&lt;p&gt;开始调研用deep learning做分词，基本把rnnlm看了一遍。最初的思路是用语言模型做分词，结果做出来就让人没什么信心，后来又把思路换成用embedding做semi-supervised，也没什么信心。当时觉得主要问题在embedding结果对于分词模型不是线性作用的。也想过用c&amp;amp;w直接做一个分词的神经网络，不过后来看ccl好像有中科院的同学用实现了这个思路，不过效果似乎也让人没什么信心。不知道dl是不是不适合参加自然语言处理的battle against state-of-the-art。&lt;/p&gt;

&lt;p&gt;断断续续地准备英语。&lt;/p&gt;

&lt;p&gt;这两个月中做的另外一件事是机房上新设备。配上机器，电路改造，修空调，反正很少能安心下来看看书或者读读论文，跑实验也总出错。&lt;/p&gt;

&lt;h4 id="section-10"&gt;5月-6月&lt;/h4&gt;
&lt;p&gt;面对一坨考试，终于撑不住了。取消的5月的G。不过考试还是成功考出两科60分。&lt;/p&gt;

&lt;h4 id="section-11"&gt;6月-8月&lt;/h4&gt;
&lt;p&gt;在万念俱灰的情绪下，用一个星期整理了之前cluster帮助分词的工作。想论证怎样做字的表示才能帮助分词任务，结论是对单个字做不靠谱，对字聚类时要考虑字的上下文信息。这个水文投出去后重构了正华师兄的依存句法分析器dparser。当时估计工作量大概是一个月，1万行代码以下。但是实际做起来发现可以顺势将ltp里面的其他模块也重构一下。结果就是重写了本科毕设的序列标注统一框架，整个项目下来有1.7万行代码。还有9K字的文档，再加上写了python和ruby两个版本的client和一部分web页面。总之这两个月彻底做了一只代码狗。&lt;/p&gt;

&lt;p&gt;6月末托福二战，再次准备十天，再次挂逼，考了个什么都不能做的93。基本是死了出国这条心了。
再一件事是实验室网站被挂马，被网络中心关站了。没办法用静态页生成器重构了主页，迁移+升级了服务器，跑了几趟网络中心。现在还有一些服务没恢复，拙计。&lt;/p&gt;

&lt;p&gt;这一年，基本上是在一种忙碌而压抑的状态中度过。没怎么过过周末，也没时间去运动。体重又回到了110斤以下，可悲的是肚子好像有胖的趋势。前个周末，爸妈来哈尔滨，幸亏没买到车票。因为那天综合楼停电，结果来电后机房就烧到50度了。若是他们来，恐怕只能把他们撂旅馆里一天了。&lt;/p&gt;

&lt;p&gt;有的时候，我也不知道想过什么应该怎样生活。也没时间去想一想，反正一直被无形的力量推动，不由自主。&lt;/p&gt;

&lt;p&gt;他们大四的毕业那会儿，我有几次被三点唱歌的醉汉们吵醒。只好去公寓平台的椅子里乘凉，有时看天色由黑转灰，继而一片青蓝，觉得很陌生。&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>脱臼</title>
    <link rel="alternate" href="http://yjliu.net/blog/2013/07/21/luxation.html"/>
    <id>http://yjliu.net/blog/2013/07/21/luxation.html</id>
    <published>2013-07-21T04:28:17Z</published>
    <updated>2014-02-23T15:05:00+08:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;昨天睡觉前，躺在床上打了个哈欠，结果一不小心下巴脱臼了。
&lt;a id="more"&gt;&lt;/a&gt;&lt;a id="more-819"&gt;&lt;/a&gt;
下巴脱臼已经不是一次两次了。第一次脱臼好像是由于吃苹果张太大口，后来则以打哈欠打脱臼了居多。我把这种病症归咎于基因，因为印象中我妈好像也脱臼过。可能是骨骼的构造不合适，也可能是韧带的力度不够，不过这些一定是由基因决定的。由于接受了这种设定，我也就比较释然，重来没埋怨过苹果或者是哈欠。
出于多次脱臼的经验，我已经学会了一些基本的救治手段。一般是将下巴向左稍用力顶一下，感觉头骨错动，也可能会听到咯噔一声。如果运气好，脱臼就归位了。运气不好就多试几次，总有那么一次成功的。
我对自己的身体还有自己的技术都是有一定信心的，所以躺在床上左手扶头，右手扶下巴，向左用了一下力。可惜没出现咯噔一声。我知道我失败了，休息了几秒，换了个姿势，又向左使了一下力，当然还是失败。“或许是躺着不方便”，这么想后，我便坐了起来，哈尔滨夏夜的凉风慢悠悠地从窗子吹了进来。我额头上有汗，所以敏锐地捕捉到了这阵不期而遇的夏天的风。
坐起来后，我又重复了几次相同的自救策略，都以失败告终。好在我并没有着急，大学这几年唯一教会我的就是在着急之前先平静一下。平静的片刻之后，一大坨信息就冲入我的脑中：“首都机场好像发生爆炸案了”；“是不是城管最近又打死人了”；“托福口语如何提高成绩，真着急”；“昨天复华那边打架来着”；“不知道当年温岚有没有和周杰伦好过”；“女神建议我换个孙燕姿的桌面，要不要考虑一下”；“最近好像写了很多代码，都快成狗了”；“哎，活着真没意思”；“不过我还是不要告诉别人，免得看起来像个怨妇”。
在一阵激烈的对于宇宙终极问题的思考之后，我意识到“这次脱臼好像是自己弄不好了。”
而这时，寝室里另外一个哥们打起了呼噜，配着青黑天空中闪烁的繁星，显得非常有节奏。
大概又经历了一次尝试的失败，我放弃了治疗。小心翼翼穿上衣服。我要去医院，这件事不怎么严重，不必惊动其他人，我自己就够了。
“我是多么独立的一个人”， 我这么想着瞄了一眼寝室的镜子，以确保自己脱臼的嘴脸在旁人看起来不那么可憎。
然后，一切妥当后，我拖着着这副夸张的对世界惊呆了的表情走入了孤独的夜色之中。&lt;a href="http://blog.oneplus.info/wp-content/uploads/2013/07/暴走漫画-www.iPc_.me33.png"&gt;&lt;img src="http://blog.oneplus.info/wp-content/uploads/2013/07/暴走漫画-www.iPc_.me33.png" alt="" title="superize" width="118" height="145" class="aligncenter size-full wp-image-820" /&gt;&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>实现一个更快一点的hashmap</title>
    <link rel="alternate" href="http://yjliu.net/blog/2013/06/18/implementation-of-a-faster-hashmap.html"/>
    <id>http://yjliu.net/blog/2013/06/18/implementation-of-a-faster-hashmap.html</id>
    <published>2013-06-18T13:54:02Z</published>
    <updated>2014-02-23T15:05:00+08:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;这段时间在写parser，难免又碰到了特征映射的问题。去年毕设做分词、词性标注时，这部分是用&lt;code&gt;__gnu_cxx::hash_map&amp;lt;string,int&amp;gt;&lt;/code&gt;来实现的。下表显示了几种数据集条件下的特征字典规模。&lt;/p&gt;

&lt;table width="100%" border="1"&gt;
&lt;tr&gt;&lt;td&gt;数据集&lt;/td&gt;&lt;td&gt;Ctb5&lt;/td&gt;&lt;td&gt;Ctb7&lt;/td&gt;&lt;td&gt;People’s Daily&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;数据规模&lt;/td&gt;&lt;td&gt;1.8W sent.&lt;/td&gt;&lt;td&gt;4.7W sent.&lt;/td&gt;&lt;td&gt;18.4W sent.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;分词特征规模&lt;/td&gt;&lt;td&gt;203.1W&lt;/td&gt;&lt;td&gt;334.8W&lt;/td&gt;&lt;td&gt;774.9W&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;词性标注特征规模&lt;/td&gt;&lt;td&gt;158.7W&lt;/td&gt;&lt;td&gt;274.2W&lt;/td&gt;&lt;td&gt;751.3W&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;对于这个级别的数据量，在特征检索过程中，特征字典的性能已经对于整个分析器的性能产生了影响。不过，分词词性标注都是序列模型，对于序列中的每个元素，只要相应进行一次抽取就可以，特征字典的性能提高一点或者降低一点，对于整体速度的影响并不是非常明显。&lt;/p&gt;

&lt;p&gt;不过在parser中，特征检索的情况就有所不同了。
主要表现就是作为一种结构学习，parser在学习模型参数以及预测过程中，需要对序列中的每两个元素抽取特征。
假如我们有30种特征模板，30词的句子，放到词性标注任务中，需要进行&lt;code&gt;30*30=900&lt;/code&gt;次特征检索，而放到依存句法分析中，就需要进行&lt;code&gt;30*30*30=27000&lt;/code&gt;次特征检索。
所以如果特征字典能再快点，当然是好事情。&lt;/p&gt;

&lt;p&gt;另外一件让我比较不爽的事情是，c++的&lt;code&gt;hash_map&lt;/code&gt;没法很好地支持持久化。
我想把&lt;code&gt;hash_map&lt;/code&gt;当成整段内存dump到磁盘上，没可能。
只能一个key-value，一个key-value地处理。
所以呢，最理想的是有这样一种&lt;code&gt;hash map&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;是一种动态的词典&lt;/li&gt;
  &lt;li&gt;以string或者const char * 作为key&lt;/li&gt;
  &lt;li&gt;性能与&lt;code&gt;__gnu_cxx::hash_map&lt;/code&gt;相近，或者更胜一筹&lt;/li&gt;
  &lt;li&gt;能够很方便地进行持久化&lt;/li&gt;
  &lt;li&gt;不需要考虑删除操作&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="section"&gt;数据持久化&lt;/h3&gt;

&lt;p&gt;进一步分析，要对数据进行持久化，最好的办法就是把所有的数据都放到一段内存上，dump的时候，直接把这段内存写到磁盘上；load时，直接从磁盘中读一段内存。
池子一下子变成了一个超级好的选择。
stl中的string基本没法持久化，不用想了。
&lt;code&gt;const char *&lt;/code&gt;倒是不错。我们可以把所有的key都固化到一段&lt;code&gt;char *&lt;/code&gt;的buffer中。&lt;/p&gt;

&lt;p&gt;对于value的固化，其实有这样一种考虑：如果value的类型是可以固化的，比如说int、double，那么也可以用池子来存这些value，但是如果是一些自建类型，比如说类啊什么的，本来就没有很直接的持久化的方法，小店也就只能伺候不周了。&lt;/p&gt;

&lt;p&gt;池子固然是好想法，但我们考虑数据结构的动态性还要大于其性能考虑。
想让池子变得动态基本就要用到stl中的allocator的技术了。
维护一个池子长度上限以及当前长度，如果新加入的元素的规模大于池子上限，就将上限翻倍，重新给池子分配一段空间，把旧空间拷贝过去。&lt;/p&gt;

&lt;p&gt;这样的话，数据固化的问题基本可以沿着这个思路解决。简单的代码可以写成这样&lt;/p&gt;

&lt;pre class="highlight cpp"&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool_cap&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool_size&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;element_size&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
	&lt;span class="n"&gt;pool_cap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;new_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
	&lt;span class="n"&gt;Element_type&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;new_pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Element_type&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;pool_cap&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
	&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;pool_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_pool&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
	&lt;span class="k"&gt;delete&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
	&lt;span class="n"&gt;pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_pool&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;
&lt;h3 id="section-1"&gt;性能优化&lt;/h3&gt;

&lt;p&gt;虽然hashmap中hash function是很大程度上影响性能的因素，但是这也是我们不能控制的事情。
我们能够提供的大概只有给一个合理的hashtable大小，以使得碰撞不是非常剧烈。
什么样的hashtable大小比较合适，开大了浪费，开小了碰撞又多，看来又要上动态的hashtable了。
不过即便是动态的hashtable，也面临resize时机的问题，爆栈上有这样一个问题“&lt;a href="http://stackoverflow.com/questions/1603712/when-should-i-do-rehashing-of-entire-hash-table/1604428#1604428"&gt;When should I do rehashing of entire hash table?&lt;/a&gt;”，第一条答案给了一个经验性的回答，翻译整理一下：&lt;/p&gt;
&lt;blockquote&gt;首先需要明确一个量load factor的概念，这个值表示hashtable的桶的个数M和桶中元素的个数N的比值，load factor=N/M。然后看一看你所使用的hashtable的类型有关（关于load factor和hashtable类型都可以去看侯捷老师的stl源码分析的5.7.1节）。
&lt;ul&gt;&lt;li&gt;线性探测（linear probing）：load factor在60%左右时就该resize了&lt;/li&gt;
&lt;li&gt;二次探测（quadratic probing）：load factor在80%-85%时就该resize了&lt;/li&gt;
&lt;li&gt;开链（separate chaining）：load factor大于150%时就该resize了&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后，我们还会面临一个问题，就是resize到多大呢？
二倍当然是一个选择，但不一定是好选择，比如说二次探测的hashtable需要hashtable大小为质数，二倍了就不是质数了。
比较好的选择是二倍以上的质数，这个&lt;a href="http://planetmath.org/goodhashtableprimes"&gt;网址&lt;/a&gt;给出了一个hashtable size的质数表。
后来我发现stl中也有一样的质数表。&lt;/p&gt;

&lt;p&gt;具体实现resize的时机是在hashmap每次insert一个元素之后，看一看是不是符合resize的机制。
如果符合，申请一段新的hashtable，然后枚举旧的hashtable中的每个元素，把他们插入到新的hashtable中。&lt;/p&gt;

&lt;p&gt;好的，到这里我们基本对于hashtable的实现心里也有数了。
不过做出来的可能也只是hash_map的一个翻版，如何在性能上进一步提升呢？
之前有一次讨论中，陆子龙师兄说在构造hashmap时可以将高频的key有限插入hashtable，这样从概率的角度讲，高频的key就有更多更大可能性一次就被检索到。
这或许是一个优化的好思路，而且貌似gnu.trove已经实现了这种机制。&lt;/p&gt;

&lt;p&gt;为了实现这一机制，用开链的方式显得要比其他几种容易。
比方说保证高频key靠前，就只要保证在插入元素的时候维护链的有序性。
因为每个链中都不会有很多元素，所以直接用类似于交换排序的思想就好了。
维护有序性的时机主要是在某个key被重复插入后，这个key的频率增加，只要看一看这个key在链表中的前一个key是不是已经小于这个key了，如果是，就往前交换就好了。
因为每个链都比较小，而且在频率增加前这个链是有序的，所以可以在极小时间复杂度内求出算某个key的正确位置时。&lt;/p&gt;

&lt;p&gt;到这里基本上整个数据结构的设计就出来了。Hash node设计成&lt;/p&gt;

&lt;pre class="highlight cpp"&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;hash_node_t&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;public&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;__key_off&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// 存key在key_pool中的偏移量
&lt;/span&gt;  &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;__val_off&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// 存value在value_pool中的偏移量
&lt;/span&gt;  &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;__freq&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// 存频次
&lt;/span&gt;  &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;__hash_val&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// 存hashvalue，用于加速
&lt;/span&gt;  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;__next_off&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// 存后继节点在node_pool中的偏移量
&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;因为池子的地址会动态变化，不能直接在hash node中保存指针，存偏移量就没有问题了。&lt;/p&gt;

&lt;p&gt;一共有三个pool，一个存key，一个存value，一个存hash node，另外一个指针数组（或者偏移量数组）存hashtable的头结点，就设计成如下图那样了。&lt;/p&gt;

&lt;p&gt;&lt;a href="http://blog.oneplus.info/wp-content/uploads/2013/06/structure.jpg"&gt;&lt;img src="http://blog.oneplus.info/wp-content/uploads/2013/06/structure.jpg" alt="Hash Table的数据结构" title="structure" width="500" class="aligncenter size-full wp-image-805" /&gt;&lt;/a&gt;
我实现的代码放在&lt;a href="https://github.com/Oneplus/libutilities/blob/master/src/smartmap/smartmap.hpp"&gt;这里&lt;/a&gt;&lt;/p&gt;

&lt;h3 id="section-2"&gt;测试&lt;/h3&gt;

&lt;p&gt;为了证明这个hashmap的性能，我进行了一个简单的benchmark。数据完全是模拟现实情境中首先构造特征字典，然后进行特征检索。benchmark的方法参考了&lt;a href="incise.org/hash-table-benchmarks.html"&gt;这篇博客&lt;/a&gt;中的方法。评价的大体过程是首先用一个有重复元素的key字典构建字典，然后用一个比较大规模的key集合去检索他的value。评价中采用了三个数据集，分别是ctb5，ctb6的分词特征集，和cdt的一阶句法特征集。key集是构造特征空间时用到的key，retrieve集是全特征集。各个数据的统计如下表所示&lt;/p&gt;

&lt;table width="100%" border="1"&gt;
&lt;tr&gt;&lt;td&gt;data set&lt;/td&gt;&lt;td&gt;# of keys&lt;/td&gt;&lt;td&gt;# of unique keys&lt;/td&gt;&lt;td&gt;# retrieve entries&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;CTB5&lt;/td&gt;&lt;td&gt;12.89M&lt;/td&gt;&lt;td&gt;2.2M&lt;/td&gt;&lt;td&gt;77.3M&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;CTB6&lt;/td&gt;&lt;td&gt;16.91M&lt;/td&gt;&lt;td&gt;2.7M&lt;/td&gt;&lt;td&gt;101.5M&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;CDT&lt;/td&gt;&lt;td&gt;55.6M&lt;/td&gt;&lt;td&gt;5.2M&lt;/td&gt;&lt;td&gt;198.9M&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;参与对比的hashmap有__gnu_cxx::hash_map，google_sparse_hash，google_dense_hash，几种hashtable都做了类似的封装。benchmark用到的代码地址在&lt;a href="https://github.com/Oneplus/libutilities/tree/master/benchmark/smartmap"&gt;这里&lt;/a&gt;。运行的时候用&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;nice -n-20 ionice -c1 -n0 python bench.py
&lt;/pre&gt;
&lt;p&gt;保证了进程的优先级。&lt;/p&gt;

&lt;p&gt;实验在xeon5650 2.67GHz的服务器上进行，gcc版本是比较老的4.1.2。&lt;/p&gt;

&lt;p&gt;在实验数据集上，时间效率和内存效率分别如下表显示。&lt;/p&gt;

&lt;p&gt;&lt;a href="http://blog.oneplus.info/wp-content/uploads/2013/06/speed.png"&gt;&lt;img src="http://blog.oneplus.info/wp-content/uploads/2013/06/speed.png" alt="" title="speed benchmark" width="500" class="aligncenter size-full wp-image-810" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://blog.oneplus.info/wp-content/uploads/2013/06/memory.png"&gt;&lt;img src="http://blog.oneplus.info/wp-content/uploads/2013/06/memory.png" alt="" title="memory benchmark" width="500" class="aligncenter size-full wp-image-811" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;总体上来讲是达到了我的期望。下一步打算把这个模块并入LTP中，期望还能再进一步优化！&lt;/p&gt;

&lt;h3 id="section-3"&gt;参考资料&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://book.douban.com/subject/1110934/"&gt;STL源码分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/1603712/when-should-i-do-rehashing-of-entire-hash-table/1604428#1604428"&gt;When should I do rehashing of entire hash table?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://incise.org/hash-table-benchmarks.html"&gt;Hash Table Benchmarks &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;话说好久没写博客了。&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>小记博客重组织</title>
    <link rel="alternate" href="http://yjliu.net/blog/2012/10/13/restruct-my-blog.html"/>
    <id>http://yjliu.net/blog/2012/10/13/restruct-my-blog.html</id>
    <published>2012-10-13T13:58:33Z</published>
    <updated>2014-02-23T15:05:00+08:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;&lt;a href="http://oneplus.info"&gt;oneplus.info&lt;/a&gt;这个域名和它使用的主机空间是我在2011年初买下的。到现在，就快有两年的时间了。两年之间，这个里产生了38篇博文，接受了2.4万次PV，其中《&lt;a href="http://blog.oneplus.info/archives/535"&gt;哈工大男女比例调研报告&lt;/a&gt;》和《&lt;a href="http://blog.oneplus.info/archives/455"&gt;关于一个点歌社交网络的构想&lt;/a&gt;》两篇得到了豆瓣九点首页的推荐。总的来讲，我对博客中提供的内容还是比较用心。&lt;/p&gt;

&lt;p&gt;虽然这个博客的一直以来的表现也没什么差错，但是我却在很早以前就产生了重新组织网站结构的想法。建站之初，没什么经验（现在我也没什么经验），直接把wordpress安装在web根目录public_html下，oneplus.info的域名也直接定位到博客上。到了现在，觉得有必要在这里加一个主页，把原来的博客移到blog.oneplus.info的域名下。出于这个考虑，这周在业余时间里完成了这两项工作。&lt;/p&gt;

&lt;h3 id="wordpress"&gt;WordPress迁移&lt;/h3&gt;

&lt;p&gt;在迁移之前，我的&lt;code&gt;public_html&lt;/code&gt;是这样的：&lt;/p&gt;

&lt;p&gt;&lt;a href="http://blog.oneplus.info/wp-content/uploads/2012/10/before.jpg"&gt;&lt;img src="http://blog.oneplus.info/wp-content/uploads/2012/10/before.jpg" alt="" title="before" width="399" height="282" class="aligncenter size-full wp-image-764" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;我希望它变成这样，&lt;/p&gt;

&lt;p&gt;&lt;a href="http://blog.oneplus.info/wp-content/uploads/2012/10/after.jpg"&gt;&lt;img src="http://blog.oneplus.info/wp-content/uploads/2012/10/after.jpg" alt="" title="after" width="398" height="152" class="aligncenter size-full wp-image-765" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;并且可以通过blog.oneplus.info来访问。&lt;/p&gt;

&lt;p&gt;要实现上面的效果，首先要做的工作是把blog.oneplus.info解析到主机的IP上。
这个只需要在DNS服务商处给主域添加一个A记录，使得HOST_NAME为blog.oneplus.info的http请求发送给我的主机。
我的DNS服务商是Godaddy，在Domain Manager面板上添加记录如下。&lt;/p&gt;

&lt;p&gt;&lt;a href="http://blog.oneplus.info/wp-content/uploads/2012/10/godaddy.jpg"&gt;&lt;img src="http://blog.oneplus.info/wp-content/uploads/2012/10/godaddy.jpg" alt="" title="godaddy" width="550" class="aligncenter size-full wp-image-766" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;添加后大概一个小时就生效了。&lt;/p&gt;

&lt;p&gt;在DNS服务的工作做完后，要做的是使得主机能够处理这个请求。
通过Google发现，这个问题大致可以通过三种途径解决，它们分别是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;配置apache服务器，添加Virtual Host&lt;/li&gt;
  &lt;li&gt;通过mod_rewrite模块把blog相关的请求重定向到blog.oneplus.info/blog/下&lt;/li&gt;
  &lt;li&gt;在cpanel中添加子域&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里面，对于用cpanel管理的主机，由于用户不能接触httpd，第一种方法不能实现。
第二种方法有点麻烦，具体做法可以google”.htaccess”、”重定向”、”二级域名”这几个关键字。
第三种方法最简单，只要在cpanel的子域一项中添加一个名为blog.oneplus.info的子域就好了，非常傻瓜。&lt;/p&gt;

&lt;p&gt;在主机可以处理blog.oneplus.info的请求之后，下一步就是wordpress的搬家了。
由于，我使用的wordpress版本是3.3.1，而且是站内移动，搬家这件事就变得非常简单。
具体做法是在&lt;code&gt;设置-&amp;gt;;常规&lt;/code&gt;中将&lt;code&gt;WordPress地址&lt;/code&gt;和&lt;code&gt;站点地址&lt;/code&gt;都写成blog.oneplus.inf
o。保存之后，站点会暂时坏掉。不过把wordpress相关的文件移动到blog文件夹下，修改就生效了。&lt;/p&gt;

&lt;p&gt;进行这些操作后，blog.oneplus.info便可以正常访问了。但是，还有一个问题是，博客中有一些图片的链接还指向blog.oneplus.info/wp-content/，科学的做法是把数据库的导出，然后把所有blog.oneplus.info改成blog.oneplus.info再导入。&lt;/p&gt;

&lt;p&gt;这些都做完了，博客的迁移工作基本就完成了。整个过程都没什么难度，但是我忽略了最早的一步，白白浪费了一个晚上的时间。&lt;/p&gt;

&lt;h3 id="feed"&gt;Feed重定向&lt;/h3&gt;

&lt;p&gt;博客迁移之后，我的博客从下面几个方面会受到影响：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;博客的订阅&lt;/li&gt;
  &lt;li&gt;搜索引擎排名&lt;/li&gt;
  &lt;li&gt;wumii的喜欢按钮&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于写博客的人，第二项虽然很重要，但是能做的其实不太多（而且我挺反感SEO的，虽然我是学信息检索的）。
所以，服务好自己的订阅用户才是要紧事。&lt;/p&gt;

&lt;p&gt;现在的情景是由于feed的输出地址发生了改变，原来通过blog.oneplus.info/feed进行的订阅失效了。
打开Google Reader，查看自己博客的订阅，发现Statistic中显示Parsing Error。
好在主域还在我的手里，只要把blog.oneplus.info/feed的请求重定向到blog.oneplus.info/feed就行了。&lt;/p&gt;

&lt;p&gt;这里要用到前面说过的比较麻烦的&lt;code&gt;mod_rewrite&lt;/code&gt;。做法是在web根目录下的.htaccess中添加重写条件和重写规则。现在我对feed的重写规则是这样的&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;RewriteCond %{HTTP_HOST} ^blog.oneplus.info$
RewriteCond %{REQUEST_URI} ^/feed$ [NC]
RewriteRule .* http://blog.oneplus.info/feed [NC,L,R=301]
&lt;/pre&gt;&lt;p&gt;它的含义是把所有主机名为blog.oneplus.info，URI为/feed的请求都重定向到blog.oneplus.info/feed下。&lt;/p&gt;

&lt;p&gt;添加完重写规则后可以通过访问blog.oneplus.info/feed来测试一下重写规则是否生效。
还有一些其他网站提供mod_rewrite的测试，比如说&lt;a href="http://martinmelin.se/rewrite-rule-tester/"&gt;这里&lt;/a&gt;，重定向失败的话可以把.htaccess投到这个网站中，找些样例测试一下。&lt;/p&gt;

&lt;p&gt;至于第三项，我倒确实把男女比那篇的一百多个“喜欢”给丢了，不过I don’t care&lt;/p&gt;

&lt;h3 id="section"&gt;主页&lt;/h3&gt;

&lt;p&gt;完成博客迁移后，我发觉应该给blog.oneplus.info写一个主页。最后决定在里面写一个个人简介（留着吹牛用）。
这次，我想尝试一下css框架（平时在实验室里也没机会），于是把bootstrap、blueprint、foundation几个框架都试了试。最后还是决定用bootstrap，原因是我在网上找到了它对ie6做的patch。
至于为什么要兼容ie6，这篇豆瓣&lt;a href="http://www.douban.com/note/241422302/"&gt;日记&lt;/a&gt;记录了原因。&lt;/p&gt;

&lt;p&gt;后来我觉得应该在主页中加一个最近发布的博客。
直接查库当然是个好选择，但是我还想在ir.hit.edu.cn/~yjliu/上面做个镜像。
所以在主机上写了一个小php - query来查库。
这样，blog.oneplus.info和ir.hit.edu.cn/~yjliu/都可以通过查这个网页获得最近的文章。
不过，我的主机在国外，国内访问速度慢，而且这个网页的更新频率实在不高。做cache是必须的。&lt;/p&gt;

&lt;p&gt;起初，我想了好久如何在query.php上做cache，后来才发现，最应该做cache的是主页这一端。所以在主页中添加了下面的代码：&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;$cache = new Cache(3600, &amp;amp;quot;some_path&amp;amp;quot;);

$key = &amp;amp;quot;last_post&amp;amp;quot;;
$values = $cache-&amp;amp;gt;get( $key );

if ($values == false) {
	$page = '';
	$handler = fopen('some_url', 'r');

	while(!feof($handler)){
		$page .= fread( $handler, 1048576 );
	}
	fclose( $handler );
	
	$values = $page;
	$cache-&amp;amp;gt;put( $key, $values );
	echo $values;
} else {
	echo $values;
}
&lt;/pre&gt;
&lt;p&gt;对于主页的请求，先去看看cache过没过期，没过期就直接返回cache的结果，这样可以减少不少网络传输。其中，cache类我是参考&lt;a href="http://www.mangguo.org/the-simple-php-cache-class/"&gt;这篇&lt;/a&gt;的。&lt;/p&gt;

&lt;h3 id="section-1"&gt;总结&lt;/h3&gt;

&lt;p&gt;至此，博客的重组织工作告于段落。现在可以通过&lt;a href="http://www.oneplus.info"&gt;www.oneplus.info&lt;/a&gt;访问我的主页，也可以通过&lt;a href="http://blog.oneplus.info"&gt;blog.oneplus.info&lt;/a&gt;访问博客。
虽然还有一些想做的工作，不过我还有别的事情，不能在这上面花太多时间，就这样吧。&lt;/p&gt;

&lt;p&gt;PS: 这篇的另一目的是测试一下Feed输出是否正常。&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>实现一个线程安全的logging库</title>
    <link rel="alternate" href="http://yjliu.net/blog/2012/10/06/the-implementation-of-a-thread-safe-logging-library.html"/>
    <id>http://yjliu.net/blog/2012/10/06/the-implementation-of-a-thread-safe-logging-library.html</id>
    <published>2012-10-06T04:33:27Z</published>
    <updated>2014-02-23T15:05:00+08:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;h3 id="introduction"&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Log是用来记录程序事件的一系列打印信息，和调试时的printf大法有点像。Log和我所接触的工作关系还是比较密切的。比方说，打印一下模型的加载时间、句子的解析速度、开发集上准确率什么的。由于接下来一段时间的工作需要写多线程，网上的logging库又不怎么习惯，所以计划造一个Log库的轮子。这篇文章中大概会讨论下面两方面内容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;用singleton模式实现logging库&lt;/li&gt;
  &lt;li&gt;Singleton模式的线程安全&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="singleton"&gt;Singleton&lt;/h3&gt;

&lt;p&gt;单件(Singleton)是设计模式的一种。如果你的程序中有某个类在程序整个的生命周期中只能被实例化一次，那么这个类就可以用单件模式来实现。直白一点说，有时候单件扮演了和全局变量类似的角色。在实际应用场景中，我们的程序中只被实例化的例子有很多，比如说：存储配置项的类。但实际并不是所有符合单件模式情景的类都要用单件来实现，不过这个已经超出本文讨论范围了。
从OO的视角看打印log的对象(Logger)也具有只被实例化一次的特点。所以，用单件模式来实现Logger问题不大。&lt;/p&gt;

&lt;p&gt;首先把设计模式书上的代码抄一遍：&lt;/p&gt;

&lt;pre class="highlight cpp"&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Singleton&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="k"&gt;public&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
       &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;Singleton&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;getInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
   &lt;span class="k"&gt;private&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
       &lt;span class="n"&gt;Singleton&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
       &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;Singleton&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;instance&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;然后，要做的就是往里面填一个打印log的函数，这个也是很容易实现的。实现后的效果如下。&lt;/p&gt;

&lt;pre class="highlight cpp"&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;logger&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;public&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;get_logger&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_instance&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;_instance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;_instance&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;write_log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"%%levelname%% "&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"log: %d&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;private&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;_instance&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;protected&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;

&lt;span class="n"&gt;logger&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;_instance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;get_logger&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;write_log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;当然也可以用变长参数和宏函数配合把这个做得有点酷，不过那些也不是本文要讨论的。
到这里，使用Singleton实现logger的任务已经完成得差不多了。接下来要做的是使上面的代码线程安全。&lt;/p&gt;

&lt;h3 id="section"&gt;线程安全&lt;/h3&gt;

&lt;p&gt;相比解释线程安全(Threads Safety)的概念，我觉得说明线程不安全更加容易。就拿前面说到的logger做例子。如果我开若干个线程，每个线程调用&lt;code&gt;logger::get_logger()-&amp;gt;write_log(tid);&lt;/code&gt;搞不好就会出现如下图的情况&lt;/p&gt;

&lt;p&gt;&lt;a href="http://blog.oneplus.info/wp-content/uploads/2012/10/thread_safety.png"&gt;&lt;img src="http://blog.oneplus.info/wp-content/uploads/2012/10/thread_safety.png" alt="" title="thread_safety" width="266" height="52" class="aligncenter size-full wp-image-728" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里就出现两个线程都向stderr打印，导致打印信息混乱了。如果write_log函数中做更复杂的操作，出现这种混乱的可能性会变得更大。造成这一现象的原因就是多个线程抢占同一文件句柄，是生产者消费者问题的一个具体情境。解决方法就是给write_log上互斥锁。把write_log函数改成下面的样子就好了。&lt;/p&gt;

&lt;pre class="highlight cpp"&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;write_log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;EnterCriticalSection&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"%%levelname%% "&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"log: %d&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;LeaveCriticalSection&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;不过，给write_log上锁也并不能完全保证logger线程安全，另一个非常隐蔽资源抢占会发生在单件实例化的那个时间上。如果logger并没被初始化，并且又有多个线程同时要去初始化它，而在初始化时发生上下文切换，那么这个logger就会被实例化多次。&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.codeproject.com/Articles/96942/Singleton-Design-Pattern-and-Thread-Safety"&gt;这里&lt;/a&gt;提供了三种解决方法，第一种是直接在判断单件是否被实例化前加锁，代码如下：&lt;/p&gt;

&lt;pre class="highlight cpp"&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;get_logger&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;EnterCriticalSection&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_instance&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;_instance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;LeaveCriticalSection&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;_instance&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;由于加互斥锁是一件比较耗时的工作，每次get_logger时都调用会加锁，解锁，程序的速度会受到影响。总之，这种方法是比较不赞的。&lt;/p&gt;

&lt;p&gt;第二种是在程序一开始就将它实例化（这种方法给人感觉也不怎么好）。弊端是如果这个单件在整个程序生命中都没有被调用，那么这次实例化就浪费了。当然浪费掉的还包括一些系统资源。&lt;/p&gt;

&lt;p&gt;第三种方法在第一种方法上进行改进，把加锁放在if判断里面，或者说在加锁外放一层if判断，代码是这样的：&lt;/p&gt;

&lt;pre class="highlight cpp"&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;get_logger&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_instance&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;EnterCriticalSection&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_instance&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;_instance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;LeaveCriticalSection&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;_instance&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;这种方法的好处是避免了每次get_logger都加锁，不过在某些情景上和第一种方法是一样的。
### printf
在实验过程中，我发现如果write_log只调用一次printf，并不会出现前面谈线程安全时的输出混乱。查了一下发现，printf本身具有操作原子性。所以，如果write_log函数只由一个printf组成，那一处的锁也可以忽略。
最后形成的代码放在github的&lt;a href="https://github.com/Oneplus/libutilities/tree/master/src/logging"&gt;这里&lt;/a&gt;。
牢骚几句，感觉操作系统很多基础知识都还给sunner了，罪过。&lt;/p&gt;

&lt;h3 id="section-1"&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://computing.llnl.gov/tutorials/pthreads/"&gt;POSIX Threads Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.codeproject.com/Articles/96942/Singleton-Design-Pattern-and-Thread-Safety"&gt;Singleton Design Pattern and Thread Safety&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/467938/stdout-thread-safe-in-c-on-linux"&gt;stdout thread-safe in C on Linux?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
  </entry>
  <entry>
    <title>最大熵模型的简单实现</title>
    <link rel="alternate" href="http://yjliu.net/blog/2012/07/22/easy-implementation-on-maxent.html"/>
    <id>http://yjliu.net/blog/2012/07/22/easy-implementation-on-maxent.html</id>
    <published>2012-07-22T13:58:16Z</published>
    <updated>2014-02-23T15:05:00+08:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;谈到最大熵，真的是一个妇孺皆知老少咸宜的好模型。
而且，网上确实已有大批量的论文、笔记、幻灯片介绍最大熵。
所以，这篇的重点放在如何实现一个简单的最大熵分类器上。&lt;/p&gt;

&lt;h3 id="section"&gt;最大熵模型推导&lt;/h3&gt;

&lt;p&gt;机器学习的任务是从数据中学习知识。
我们做分类问题，看到的数据往往是每个实例对应一个类别。
比如说词性标注中一个词对应一个标注。
为了下面讨论的方便，将类别称为Outcome，将每个实例的上下文环境叫做Context。
实例被称为Event，一个实例是Outcome与Context的二元组。&lt;/p&gt;

&lt;p&gt;为了表示数据，我们从数据中抽取了一系列的特征。特征的全体可以看做是n个特征函数组成的一个集合。每个特征函数可以是从Context到0-1的二值函数。比方说这样的
&lt;script type="math/tex"&gt;% &lt;![CDATA[

f(x,y)=\begin{aligned} 1 &amp; if\ x="is"\ and\ y=v \\ 0 &amp; otherwise \end{aligned}
 %]]&gt;&lt;/script&gt;
一组特征函数就将Context从上下文空间映射到特征空间上了。&lt;/p&gt;

&lt;p&gt;现在，我们观察到一组数据集，通过简单的统计可以知道任意一个Context x和Outcome y的组合的联合概率。有了联合概率，可以计算观察到的某一特征函数f的期望，也就是 $E_{ref}(f)= \sum_{x,y}{\tilde{p}(x,y)f(x,y)}$，称为观察期望/经验期望。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;假设我们有一个模型给出p(y&lt;/td&gt;
      &lt;td&gt;x)的值，那么我们可以从这个模型的角度求出这个特征函数的期望，即$ E_q(f)=\sum_{x,y}{\tilde{p}(x)p(y&lt;/td&gt;
      &lt;td&gt;x)f(x,y)}$，称为模型期望。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;我们希望我们的模型能够很好地反应这些数据中蕴含的现象。那么从模型角度看到的f的期望就应该等于从数据观察到的f的期望。也就是$ E_q(f)=E_{ref}(f) $&lt;/p&gt;

&lt;p&gt;假设我们有n个特征函数，那么我们就有n组等式$ E_q(f_i)=E_{ref}(f_i), i \in {1,2,…,n}$。&lt;/p&gt;

&lt;p&gt;假设我们有那么那么多的模型，也可以认为是概率分布。他们组成一个空间$ \mathcal{P} $，而满足上面一系列特征函数期望构成的等式的概率分布构成了$ \mathcal{P}$的一个子集&lt;/p&gt;

&lt;script type="math/tex; mode=display"&gt;
\mathcal{C}=\{p|p\in \mathcal{P} \quad and \ E_q(f_i)=E_{ref}{f_i},\ i \in (1,2,...,n)\}
&lt;/script&gt;

&lt;p&gt;现在要找一个合适的模型描述数据，也就是在$ \mathcal{P} $中搜索一个p。
然后这时，熵突然出现了（- -b这里我也不知道怎么写，原谅我吧），从最大熵的角度来看，这个模型需要满足上面一系列特征函数期望构成的等式（换句话讲，是一些列一系列特征函数期望构成的约束），同时尽可能将可能性均匀地非配到不确定的上下文情况中。&lt;/p&gt;

&lt;p&gt;对于一个模型p，它的熵的定义是这样$ H(p)=-\sum_{x,y}\tilde{p}(x)p(y|x)\log{p(y|x)}$。
熵越大，可能性就越平均地被分配，因而我们的最终目标是最大化一个模型的熵。
而由于有前面的约束等式，这个问题变成了一个有约束的最优化问题
&lt;script type="math/tex"&gt;
find p* = arg{max_{p\in C}{H(p)}}
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;然后引入拉格朗日乘子$\lambda$，将等式约束的优化转换成无约束的最优化，得到
&lt;script type="math/tex"&gt;
\Lambda(p,\lambda)=H(p)+\lambda\sum_i(E_q(f_i)-E_{ref}(f_i))
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;求导解p，得到一个与$\lambda$有关的函数
&lt;script type="math/tex"&gt;
p_{\lambda}(y|x)=\frac{1}{Z_{\lambda}(x)}\exp(\sum_i{\lambda_i f_i(x,y)})
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;其中，$Z&lt;em&gt;\lambda(x)=\sum_y{\exp(\sum_i \lambda_i f_i(x,y))}$ 。
将$p&lt;/em&gt;{\lambda}$带入无约束优化的式子中，得到
&lt;script type="math/tex"&gt;
\Psi(\lambda)=\Lambda(p,\lambda)=-\sum_x\tilde{p}\log{Z_\lambda(x)}+\sum_i{\lambda_i E_{ref}(f_i)}
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;推导到这里，我们基本得到了算法要做的工作，就是从数据中估计出一个特征函数的权向量$\lambda$。最大化$ \Psi(\lambda) $。&lt;/p&gt;

&lt;p&gt;注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Psi(\lambda)$等于经验分布的最大似然估计，也就是$\Psi(\lambda)=L_p(\Lambda)$&lt;/li&gt;
&lt;li&gt;上面的求导过程被忽略了&lt;/li&gt;
&lt;li&gt;为什么最大化$\Psi(\lambda)$是正确的，这里涉及到原问题和对偶问题，还有KTT条件。我觉得应该是写不明白，所以直接跳过，不影响我们实现最大熵模型。&lt;/li&gt;&lt;/ul&gt;

&lt;h3 id="section-1"&gt;最大熵模型的实现&lt;/h3&gt;

&lt;p&gt;要算$ \lambda $，解析解肯定是行不通的。对于最大熵模型对应的最优化问题，GIS，lbfgs，sgd等等最优化算法都能解。相比之下，GIS大概是最好实现的。算法的流程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化$ \lambda=0$&lt;/li&gt;
&lt;li&gt;循环$ \lambda_i^{(t+1)}=\lambda_i^{(t)}+\frac{1}{C}\log{\frac{E_{ref}(f_i)}{E_q(f_i)}} $&lt;/li&gt;
&lt;li&gt;重复2到收敛&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中，$ C=\max&lt;em&gt;{x,y}\sum&lt;/em&gt;{i=1}^n{f_i(x,y)} $。&lt;/p&gt;

&lt;p&gt;根据上面算法，在最大熵模型的实现过程中，我们需要计算的值包括经验期望$E_{ref}(f)$和各轮迭代过后的模型期望$E_q(f)$。&lt;/p&gt;

&lt;p&gt;经验期望$ E&lt;em&gt;{ref}(f_i)= \sum&lt;/em&gt;{x,y}{\tilde{p}(x,y)f_i(x,y)} $，求$ E_{ref}(f_i)$只需要统计训练数据中符合$f_i$的(x,y)二元组的个数，然后除以训练实例的个数N。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;模型期望需要首先求p(y&lt;/td&gt;
      &lt;td&gt;x)。这个条件概率可以通过简单地将所有(x,y)符合的$f_i$和对应的参数$\lambda_i$乘起来后相加。归一化因子是各个Outcome y的p(y&lt;/td&gt;
      &lt;td&gt;x)的和。在求得p(y&lt;/td&gt;
      &lt;td&gt;x)后，要求$E_q(f_i)$，只需要枚举所有符合$f_i$的(x,y)对应的p(y&lt;/td&gt;
      &lt;td&gt;x)，乘以Context x出现的次数再除以N就可以。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;模型期望有了，经验期望有了，把他们一股脑儿放到算法里面去迭代就好了。&lt;/p&gt;

&lt;p&gt;当然，到这里我们完全可以写出一个最大熵分类器。
不过，需要注意的一点，在上文也提到最大化$\Psi(\lambda)$实际是在做最大似然估计。
谈到最大似然估计，就不可避免想到了过拟合问题。
如果训练数据和测试数据的偏置比较大。
我们训练的模型很可能无法再测试数据上取得比较好的效果。
解决这一问题的一般套路是用给待估计的参数一个先验分布做MAP，这里也不例外。
如果假设$\lambda$服从Gauss分布，优化的目标函数就变成了$L_{\tilde{p}}^{‘}(\Lambda)=L_p(\Lambda)+\sum_i{\log{\frac{1}{\sqrt{2\pi\sigma_i^2}}}}-\frac{\lambda_i^2}{2\sigma_i^2}$&lt;/p&gt;

&lt;p&gt;在使用高斯先验平滑模型后，GIS的更新变成解$ E_{ref}(f_i)=E_q(f_i)e^{C\delta_i}+\frac{\lambda_i+\delta_i}{\sigma_i^2} $。
其中，$\lambda_i$是本轮迭代的参数，$\delta_i$是更新$\lambda_i$的增量。
上面的方程是没有解析解的，不过可以用牛顿法解除数值解。&lt;/p&gt;

&lt;h3 id="section-2"&gt;补充&lt;/h3&gt;

&lt;p&gt;本来是想看一看lbfgs的，不过那个最优化的库实在是太复杂了，所以基本用的时候就当黑盒了。而且，从maxent的代码来看，其用法和GIS类似，都需要做求$ E_{ref}(f_i)$和$E_q(f_i)$的工作。&lt;/p&gt;

&lt;p&gt;在了解了最大熵模型的一些实现细节后，我们就可以动手去实现一个最大熵分类器了。我用python写了一个GIS+binary feature版的maxent，去掉读写模型文件等外围模块，大概300行多一点。总体来讲是非常容易的。同时推荐阅读一下张乐的maxent toolkit，个人感觉写得非常好。&lt;/p&gt;

&lt;h3 id="section-3"&gt;参考&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.isi.edu/natural-language/people/ravichan/papers/bergeretal96.pdf"&gt;A Maximum Entropy Approach to Natural Language Processing&lt;/a&gt; 个人感觉应该是最大熵必看的一篇论文。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://users.cms.caltech.edu/~weixl/research/read/summary/MaxEnt2.ppt"&gt;最大熵模型与自然语言处理&lt;/a&gt; 感觉这个slide应该是基于上一篇做的，配合着看应该效果很好。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://work-tmp.googlecode.com/svn/trunk/maxent/10.1.1.123.127.pdf"&gt;The Improved Iterative Scaling Algorithm: A Gentle Introduction&lt;/a&gt; 讲IIS为什么work&lt;/li&gt;
&lt;li&gt;&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.714&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Investigating GIS and Smoothing for Maximum Entropy Taggers&lt;/a&gt; 如果要实现一个maxent，这篇也是非常非常推荐的，从最基本的GIS到加高斯先验，后面还有必要的证明。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html"&gt;Maximum Entropy Modeling Toolkit for Python and C++&lt;/a&gt; 最大熵工具包，源码写得非常好看，无论从算法角度还是从软件工程角度。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nicyun/easyME"&gt;easyME&lt;/a&gt; nicyun等师兄实现的maxent toolkit的化简版，简单易懂。&lt;/li&gt;
&lt;/ul&gt;

</content>
  </entry>
</feed>
